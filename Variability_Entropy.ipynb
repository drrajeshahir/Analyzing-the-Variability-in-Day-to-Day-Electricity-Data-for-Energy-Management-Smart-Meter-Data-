{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from scipy.special import rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data processing/KOL_15mins_final_data-11839.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>dates</th>\n",
       "      <th>00:00:00</th>\n",
       "      <th>00:15:00</th>\n",
       "      <th>00:30:00</th>\n",
       "      <th>00:45:00</th>\n",
       "      <th>01:00:00</th>\n",
       "      <th>01:15:00</th>\n",
       "      <th>01:30:00</th>\n",
       "      <th>01:45:00</th>\n",
       "      <th>...</th>\n",
       "      <th>21:30:00</th>\n",
       "      <th>21:45:00</th>\n",
       "      <th>22:00:00</th>\n",
       "      <th>22:15:00</th>\n",
       "      <th>22:30:00</th>\n",
       "      <th>22:45:00</th>\n",
       "      <th>23:00:00</th>\n",
       "      <th>23:15:00</th>\n",
       "      <th>23:30:00</th>\n",
       "      <th>23:45:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KOL-22812953</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KOL-22812953</td>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KOL-22812953</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KOL-22812953</td>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KOL-22812953</td>\n",
       "      <td>2019-12-05</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      device_id       dates  00:00:00  00:15:00  00:30:00  00:45:00  01:00:00  \\\n",
       "0  KOL-22812953  2019-12-01     0.016     0.015     0.040     0.042     0.030   \n",
       "1  KOL-22812953  2019-12-02     0.020     0.019     0.017     0.009     0.009   \n",
       "2  KOL-22812953  2019-12-03     0.016     0.021     0.015     0.017     0.016   \n",
       "3  KOL-22812953  2019-12-04     0.017     0.024     0.015     0.018     0.019   \n",
       "4  KOL-22812953  2019-12-05     0.015     0.022     0.016     0.015     0.018   \n",
       "\n",
       "   01:15:00  01:30:00  01:45:00  ...  21:30:00  21:45:00  22:00:00  22:15:00  \\\n",
       "0     0.009     0.010     0.009  ...     0.016     0.015     0.016     0.016   \n",
       "1     0.010     0.009     0.010  ...     0.016     0.015     0.017     0.016   \n",
       "2     0.016     0.016     0.009  ...     0.016     0.035     0.026     0.015   \n",
       "3     0.009     0.010     0.009  ...     0.011     0.011     0.012     0.015   \n",
       "4     0.011     0.011     0.012  ...     0.016     0.016     0.016     0.015   \n",
       "\n",
       "   22:30:00  22:45:00  23:00:00  23:15:00  23:30:00  23:45:00  \n",
       "0     0.016     0.016     0.018     0.019     0.028     0.021  \n",
       "1     0.015     0.015     0.016     0.015     0.015     0.016  \n",
       "2     0.016     0.015     0.016     0.015     0.015     0.015  \n",
       "3     0.016     0.020     0.015     0.037     0.041     0.023  \n",
       "4     0.016     0.020     0.015     0.015     0.016     0.015  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20164\\3678943105.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['device_id'] = df['device_id'].str.replace(r'\\D', '')\n"
     ]
    }
   ],
   "source": [
    "df['device_id'] = df['device_id'].str.replace(r'\\D', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>dates</th>\n",
       "      <th>00:00:00</th>\n",
       "      <th>00:15:00</th>\n",
       "      <th>00:30:00</th>\n",
       "      <th>00:45:00</th>\n",
       "      <th>01:00:00</th>\n",
       "      <th>01:15:00</th>\n",
       "      <th>01:30:00</th>\n",
       "      <th>01:45:00</th>\n",
       "      <th>...</th>\n",
       "      <th>21:30:00</th>\n",
       "      <th>21:45:00</th>\n",
       "      <th>22:00:00</th>\n",
       "      <th>22:15:00</th>\n",
       "      <th>22:30:00</th>\n",
       "      <th>22:45:00</th>\n",
       "      <th>23:00:00</th>\n",
       "      <th>23:15:00</th>\n",
       "      <th>23:30:00</th>\n",
       "      <th>23:45:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22812953</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22812953</td>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22812953</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22812953</td>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22812953</td>\n",
       "      <td>2019-12-05</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  device_id       dates  00:00:00  00:15:00  00:30:00  00:45:00  01:00:00  \\\n",
       "0  22812953  2019-12-01     0.016     0.015     0.040     0.042     0.030   \n",
       "1  22812953  2019-12-02     0.020     0.019     0.017     0.009     0.009   \n",
       "2  22812953  2019-12-03     0.016     0.021     0.015     0.017     0.016   \n",
       "3  22812953  2019-12-04     0.017     0.024     0.015     0.018     0.019   \n",
       "4  22812953  2019-12-05     0.015     0.022     0.016     0.015     0.018   \n",
       "\n",
       "   01:15:00  01:30:00  01:45:00  ...  21:30:00  21:45:00  22:00:00  22:15:00  \\\n",
       "0     0.009     0.010     0.009  ...     0.016     0.015     0.016     0.016   \n",
       "1     0.010     0.009     0.010  ...     0.016     0.015     0.017     0.016   \n",
       "2     0.016     0.016     0.009  ...     0.016     0.035     0.026     0.015   \n",
       "3     0.009     0.010     0.009  ...     0.011     0.011     0.012     0.015   \n",
       "4     0.011     0.011     0.012  ...     0.016     0.016     0.016     0.015   \n",
       "\n",
       "   22:30:00  22:45:00  23:00:00  23:15:00  23:30:00  23:45:00  \n",
       "0     0.016     0.016     0.018     0.019     0.028     0.021  \n",
       "1     0.015     0.015     0.016     0.015     0.015     0.016  \n",
       "2     0.016     0.015     0.016     0.015     0.015     0.015  \n",
       "3     0.016     0.020     0.015     0.037     0.041     0.023  \n",
       "4     0.016     0.020     0.015     0.015     0.016     0.015  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['device_id'] = df['device_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dates'] = pd.to_datetime(df[\"dates\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00:00:00</th>\n",
       "      <th>00:15:00</th>\n",
       "      <th>00:30:00</th>\n",
       "      <th>00:45:00</th>\n",
       "      <th>01:00:00</th>\n",
       "      <th>01:15:00</th>\n",
       "      <th>01:30:00</th>\n",
       "      <th>01:45:00</th>\n",
       "      <th>02:00:00</th>\n",
       "      <th>02:15:00</th>\n",
       "      <th>...</th>\n",
       "      <th>21:30:00</th>\n",
       "      <th>21:45:00</th>\n",
       "      <th>22:00:00</th>\n",
       "      <th>22:15:00</th>\n",
       "      <th>22:30:00</th>\n",
       "      <th>22:45:00</th>\n",
       "      <th>23:00:00</th>\n",
       "      <th>23:15:00</th>\n",
       "      <th>23:30:00</th>\n",
       "      <th>23:45:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606998</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606999</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607000</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607001</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607002</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4607003 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00:00:00  00:15:00  00:30:00  00:45:00  01:00:00  01:15:00  01:30:00  \\\n",
       "0           0.016     0.015     0.040     0.042     0.030     0.009     0.010   \n",
       "1           0.020     0.019     0.017     0.009     0.009     0.010     0.009   \n",
       "2           0.016     0.021     0.015     0.017     0.016     0.016     0.016   \n",
       "3           0.017     0.024     0.015     0.018     0.019     0.009     0.010   \n",
       "4           0.015     0.022     0.016     0.015     0.018     0.011     0.011   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "4606998     0.000     0.000     0.010     0.000     0.000     0.000     0.000   \n",
       "4606999     0.000     0.000     0.000     0.000     0.000     0.000     0.000   \n",
       "4607000     0.050     0.050     0.050     0.050     0.050     0.050     0.150   \n",
       "4607001     0.010     0.000     0.000     0.000     0.000     0.000     0.000   \n",
       "4607002     0.000     0.000     0.000     0.000     0.000     0.010     0.000   \n",
       "\n",
       "         01:45:00  02:00:00  02:15:00  ...  21:30:00  21:45:00  22:00:00  \\\n",
       "0           0.009     0.009     0.009  ...     0.016     0.015     0.016   \n",
       "1           0.010     0.010     0.009  ...     0.016     0.015     0.017   \n",
       "2           0.009     0.009     0.009  ...     0.016     0.035     0.026   \n",
       "3           0.009     0.010     0.009  ...     0.011     0.011     0.012   \n",
       "4           0.012     0.011     0.011  ...     0.016     0.016     0.016   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "4606998     0.000     0.010     0.010  ...     0.000     0.010     0.000   \n",
       "4606999     0.010     0.000     0.000  ...     0.050     0.050     0.050   \n",
       "4607000     0.050     0.050     0.050  ...     0.010     0.000     0.000   \n",
       "4607001     0.000     0.000     0.000  ...     0.000     0.000     0.000   \n",
       "4607002     0.000     0.010     0.010  ...     0.000     0.010     0.000   \n",
       "\n",
       "         22:15:00  22:30:00  22:45:00  23:00:00  23:15:00  23:30:00  23:45:00  \n",
       "0           0.016     0.016     0.016     0.018     0.019     0.028     0.021  \n",
       "1           0.016     0.015     0.015     0.016     0.015     0.015     0.016  \n",
       "2           0.015     0.016     0.015     0.016     0.015     0.015     0.015  \n",
       "3           0.015     0.016     0.020     0.015     0.037     0.041     0.023  \n",
       "4           0.015     0.016     0.020     0.015     0.015     0.016     0.015  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "4606998     0.000     0.000     0.010     0.010     0.000     0.000     0.000  \n",
       "4606999     0.050     0.050     0.050     0.050     0.050     0.040     0.050  \n",
       "4607000     0.000     0.000     0.010     0.010     0.000     0.000     0.110  \n",
       "4607001     0.010     0.000     0.000     0.000     0.000     0.000     0.000  \n",
       "4607002     0.000     0.000     0.000     0.000     0.000     0.000     0.000  \n",
       "\n",
       "[4607003 rows x 96 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,2:98].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,2:98] = df.iloc[:,2:98].div(df.iloc[:,2:98].sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clu_met = pd.read_csv(\"Cluster_meter_num.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idmeter</th>\n",
       "      <th>clus_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22812953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22812954</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22812955</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22812956</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22812957</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idmeter  clus_num\n",
       "0  22812953         1\n",
       "1  22812954         2\n",
       "2  22812955         3\n",
       "3  22812956         4\n",
       "4  22812957         4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clu_met.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clu1_met = df_clu_met.loc[df_clu_met['clus_num'] == 1]\n",
    "df_clu2_met = df_clu_met.loc[df_clu_met['clus_num'] == 2]\n",
    "df_clu3_met = df_clu_met.loc[df_clu_met['clus_num'] == 3]\n",
    "df_clu4_met = df_clu_met.loc[df_clu_met['clus_num'] == 4]\n",
    "df_clu5_met = df_clu_met.loc[df_clu_met['clus_num'] == 5]\n",
    "df_clu6_met = df_clu_met.loc[df_clu_met['clus_num'] == 6]\n",
    "df_clu7_met = df_clu_met.loc[df_clu_met['clus_num'] == 7]\n",
    "df_clu8_met = df_clu_met.loc[df_clu_met['clus_num'] == 8]\n",
    "df_clu9_met = df_clu_met.loc[df_clu_met['clus_num'] == 9]\n",
    "df_clu10_met = df_clu_met.loc[df_clu_met['clus_num'] == 10]\n",
    "df_clu11_met = df_clu_met.loc[df_clu_met['clus_num'] == 11]\n",
    "df_clu12_met = df_clu_met.loc[df_clu_met['clus_num'] == 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[df['device_id'].isin(df_clu1_met['idmeter'])]\n",
    "df2 = df.loc[df['device_id'].isin(df_clu2_met['idmeter'])]\n",
    "df3 = df.loc[df['device_id'].isin(df_clu3_met['idmeter'])]\n",
    "df4 = df.loc[df['device_id'].isin(df_clu4_met['idmeter'])]\n",
    "df5 = df.loc[df['device_id'].isin(df_clu5_met['idmeter'])]\n",
    "df6 = df.loc[df['device_id'].isin(df_clu6_met['idmeter'])]\n",
    "df7 = df.loc[df['device_id'].isin(df_clu7_met['idmeter'])]\n",
    "df8 = df.loc[df['device_id'].isin(df_clu8_met['idmeter'])]\n",
    "df9 = df.loc[df['device_id'].isin(df_clu9_met['idmeter'])]\n",
    "df10 = df.loc[df['device_id'].isin(df_clu10_met['idmeter'])]\n",
    "df11 = df.loc[df['device_id'].isin(df_clu11_met['idmeter'])]\n",
    "df12 = df.loc[df['device_id'].isin(df_clu12_met['idmeter'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_id      280\n",
       "dates          394\n",
       "00:00:00     85063\n",
       "00:15:00     83340\n",
       "00:30:00     82096\n",
       "             ...  \n",
       "22:45:00     93119\n",
       "23:00:00     91899\n",
       "23:15:00     90517\n",
       "23:30:00     88655\n",
       "23:45:00     86697\n",
       "Length: 98, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mean = df1.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df2_mean = df2.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df3_mean = df3.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df4_mean = df4.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df5_mean = df5.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df6_mean = df6.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df7_mean = df7.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df8_mean = df8.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df9_mean = df9.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df10_mean = df10.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df11_mean = df11.iloc[:,2:98].mean(axis=0).values.tolist()\n",
    "df12_mean = df12.iloc[:,2:98].mean(axis=0).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.778061827830856"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.iloc[:,2:98].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m         d1 \u001b[38;5;241m=\u001b[39m df1_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m     71\u001b[0m         m1 \u001b[38;5;241m=\u001b[39m df1_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[1;32m---> 72\u001b[0m         df1_r_entr \u001b[38;5;241m=\u001b[39m \u001b[43mdf1_r_entr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdates\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midmeter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_d2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2d_morn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_morn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2d_daytime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_daytime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2d_pre-peak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_pre_peak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2d_peak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_peak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2d_post-peak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_post_peak\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_d2mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2mean_morn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_morn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2mean_daytime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_daytime1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2mean_pre-peak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_pre_peak1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2mean_peak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_peak1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr_entr_d2mean_post-peak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_entr1_post_peak1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdailymean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdailymean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotalcons\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotalcons\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m df1_r_entr\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster1_RE.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9045\u001b[0m, in \u001b[0;36mDataFrame.append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   8942\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8943\u001b[0m \u001b[38;5;124;03mAppend rows of `other` to the end of caller, returning a new object.\u001b[39;00m\n\u001b[0;32m   8944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9035\u001b[0m \u001b[38;5;124;03m4  4\u001b[39;00m\n\u001b[0;32m   9036\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9037\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   9038\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe frame.append method is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   9039\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed from pandas in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9042\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   9043\u001b[0m )\n\u001b[1;32m-> 9045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9088\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   9085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   9086\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, other]\n\u001b[1;32m-> 9088\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9092\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9093\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   9095\u001b[0m     combined_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   9096\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9101\u001b[0m     \u001b[38;5;66;03m# combined_columns.equals check is necessary for preserving dtype\u001b[39;00m\n\u001b[0;32m   9102\u001b[0m     \u001b[38;5;66;03m#  in test_crosstab_normalize\u001b[39;00m\n\u001b[0;32m   9103\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreindex(combined_columns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:360\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03malong the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    347\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m     objs,\n\u001b[0;32m    349\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    358\u001b[0m )\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:595\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    591\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    593\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 595\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    599\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py:232\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the relative entropy for cluster 1\n",
    "mlist = df1['device_id'].unique().tolist()\n",
    "df1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df1.loc[df1['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df1.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df1.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df1.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df1.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df1.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df1_r_entr = df1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df1_r_entr.to_csv(\"Cluster1_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 1\n",
    "mlist = df1['device_id'].unique().tolist()\n",
    "mlist = mlist[:650]\n",
    "df1_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df1.loc[df1['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df1_r_entr = df1_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df1_r_entr.to_csv(\"Cluster1_RE_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_r_entr.to_csv(\"Cluster1_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 2\n",
    "mlist = df2['device_id'].unique().tolist()\n",
    "df2_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df2.loc[df2['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df2.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df2.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df2.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df2.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df2.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df2_r_entr = df2_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df2_r_entr.to_csv(\"Cluster2_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 2\n",
    "mlist = df2['device_id'].unique().tolist()\n",
    "mlist = mlist[:375]\n",
    "df2_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df2.loc[df2['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df2_r_entr = df2_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df2_r_entr.to_csv(\"Cluster1_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df3['device_id'].unique().tolist()\n",
    "df3_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df3.loc[df3['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df3.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df3.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df3.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df3.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df3.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df3_r_entr = df3_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df3_r_entr.to_csv(\"Cluster3_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df3['device_id'].unique().tolist()\n",
    "mlist = mlist[:841]\n",
    "df3_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df3.loc[df3['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df3_r_entr = df3_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df3_r_entr.to_csv(\"Cluster3_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m r_entr1_post_peak1 \u001b[38;5;241m=\u001b[39m r_entr_post_peak1\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m############# Calculation of daily mean consumption and total consumption\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m df1_main \u001b[38;5;241m=\u001b[39m \u001b[43mdf_main\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_main\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     68\u001b[0m dailymean \u001b[38;5;241m=\u001b[39m df1_main\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m98\u001b[39m]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     69\u001b[0m totalcons \u001b[38;5;241m=\u001b[39m df1_main\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m98\u001b[39m]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1182\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getbool_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1184\u001b[0m \n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:985\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    983\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m    984\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(labels, key)\n\u001b[1;32m--> 985\u001b[0m inds \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the relative entropy for cluster 4\n",
    "mlist = df4['device_id'].unique().tolist()\n",
    "df4_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak'])\n",
    "for j in mlist:\n",
    "    df1_1 = df4.loc[df4['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df4.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df4.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df4.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df4.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df4.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df4_r_entr = df4_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df4_r_entr.to_csv(\"Cluster4_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df4['device_id'].unique().tolist()\n",
    "mlist = mlist[:502]\n",
    "df4_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df4.loc[df4['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df4_r_entr = df4_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df4_r_entr.to_csv(\"Cluster4_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_r_entr.to_csv(\"Cluster4_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m pre_peak \u001b[38;5;241m=\u001b[39m df5\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m59\u001b[39m:\u001b[38;5;241m71\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Pre-peak slot: 2pm to 5pm\u001b[39;00m\n\u001b[0;32m     54\u001b[0m peak \u001b[38;5;241m=\u001b[39m df5\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m71\u001b[39m:\u001b[38;5;241m95\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Peak slot: 5pm to 11pm\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m post_peak \u001b[38;5;241m=\u001b[39m \u001b[43mdf5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m97\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Post-peak slot: 11pm to 2am\u001b[39;00m\n\u001b[0;32m     56\u001b[0m r_entr_morn1 \u001b[38;5;241m=\u001b[39m rel_entr(l1_morn, morn)\n\u001b[0;32m     57\u001b[0m r_entr_daytime1 \u001b[38;5;241m=\u001b[39m rel_entr(l1_daytime, daytime)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:11119\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11101\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11102\u001b[0m     _num_doc,\n\u001b[0;32m  11103\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11118\u001b[0m ):\n\u001b[1;32m> 11119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10689\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  10682\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10683\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10687\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  10688\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 10689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  10690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  10691\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10641\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10631\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  10633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10636\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  10637\u001b[0m     )\n\u001b[0;32m  10638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  10639\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  10640\u001b[0m     )\n\u001b[1;32m> 10641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  10643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10020\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10016\u001b[0m ignore_failures \u001b[38;5;241m=\u001b[39m numeric_only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m  10018\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  10019\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 10020\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10021\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor(res)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  10022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1389\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m   1387\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1389\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1392\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:412\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9992\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m   9990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39m_reduce(name, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   9991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 9992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:146\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# `mask` is not recognised by bottleneck, would raise\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m#  TypeError if called\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 146\u001b[0m     result \u001b[38;5;241m=\u001b[39m bn_func(values, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# prefer to treat inf/-inf as NA, but must compute the func\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# twice :(\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_infs(result):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the relative entropy for cluster 5\n",
    "mlist = df5['device_id'].unique().tolist()\n",
    "df5_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df5.loc[df5['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df5.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df5.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df5.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df5.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df5.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df5_r_entr = df5_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df5_r_entr.to_csv(\"Cluster5_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df5['device_id'].unique().tolist()\n",
    "mlist = mlist[:341]\n",
    "df5_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df5.loc[df5['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df5_r_entr = df5_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df5_r_entr.to_csv(\"Cluster5_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 6\n",
    "mlist = df6['device_id'].unique().tolist()\n",
    "df6_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df6.loc[df6['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df6.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df6.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df6.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df6.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df6.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df6_r_entr = df6_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df6_r_entr.to_csv(\"Cluster6_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df6['device_id'].unique().tolist()\n",
    "mlist = mlist[:413]\n",
    "df6_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df6.loc[df6['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df6_r_entr = df6_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df6_r_entr.to_csv(\"Cluster6_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m l1_peak \u001b[38;5;241m=\u001b[39m df1_1\u001b[38;5;241m.\u001b[39miloc[[i],\u001b[38;5;241m71\u001b[39m:\u001b[38;5;241m95\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     50\u001b[0m l1_post_peak \u001b[38;5;241m=\u001b[39m df1_1\u001b[38;5;241m.\u001b[39miloc[[i],[\u001b[38;5;241m95\u001b[39m,\u001b[38;5;241m96\u001b[39m,\u001b[38;5;241m97\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 51\u001b[0m morn \u001b[38;5;241m=\u001b[39m \u001b[43mdf7\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m43\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Morning slot: 6am to 10am\u001b[39;00m\n\u001b[0;32m     52\u001b[0m daytime \u001b[38;5;241m=\u001b[39m df7\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m46\u001b[39m:\u001b[38;5;241m58\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Daytime slot: 11am to 02pm\u001b[39;00m\n\u001b[0;32m     53\u001b[0m pre_peak \u001b[38;5;241m=\u001b[39m df7\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m59\u001b[39m:\u001b[38;5;241m71\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Pre-peak slot: 2pm to 5pm\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:11119\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11101\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11102\u001b[0m     _num_doc,\n\u001b[0;32m  11103\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11118\u001b[0m ):\n\u001b[1;32m> 11119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10689\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  10682\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10683\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10687\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  10688\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 10689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  10690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  10691\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10641\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10631\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  10633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10636\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  10637\u001b[0m     )\n\u001b[0;32m  10638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  10639\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  10640\u001b[0m     )\n\u001b[1;32m> 10641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  10643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10020\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10016\u001b[0m ignore_failures \u001b[38;5;241m=\u001b[39m numeric_only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m  10018\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  10019\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 10020\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10021\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor(res)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  10022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1389\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m   1387\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1389\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1392\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:412\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9992\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m   9990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39m_reduce(name, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   9991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 9992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:146\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# `mask` is not recognised by bottleneck, would raise\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m#  TypeError if called\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 146\u001b[0m     result \u001b[38;5;241m=\u001b[39m bn_func(values, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# prefer to treat inf/-inf as NA, but must compute the func\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# twice :(\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_infs(result):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the relative entropy for cluster 7\n",
    "mlist = df7['device_id'].unique().tolist()\n",
    "df7_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df7.loc[df7['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df7_r_entr = df7_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df7_r_entr.to_csv(\"Cluster7_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df7['device_id'].unique().tolist()\n",
    "mlist = mlist[:1305]\n",
    "df7_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df7.loc[df7['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df7_r_entr = df7_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df7_r_entr.to_csv(\"Cluster7_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7_r_entr.to_csv(\"Cluster7_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m r_entr1_post_peak1 \u001b[38;5;241m=\u001b[39m r_entr_post_peak1\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m############# Calculation of daily mean consumption and total consumption\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m df1_main \u001b[38;5;241m=\u001b[39m df_main\u001b[38;5;241m.\u001b[39mloc[\u001b[43mdf_main\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m]\n\u001b[0;32m     68\u001b[0m dailymean \u001b[38;5;241m=\u001b[39m df1_main\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m98\u001b[39m]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     69\u001b[0m totalcons \u001b[38;5;241m=\u001b[39m df1_main\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m98\u001b[39m]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:5623\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5620\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 5623\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:74\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the relative entropy for cluster 8\n",
    "mlist = df8['device_id'].unique().tolist()\n",
    "df8_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df8.loc[df8['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df8_r_entr = df8_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df8_r_entr.to_csv(\"Cluster8_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df8['device_id'].unique().tolist()\n",
    "mlist = mlist[:691]\n",
    "df8_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df8.loc[df8['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df8_r_entr = df8_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df8_r_entr.to_csv(\"Cluster8_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8_r_entr.to_csv(\"Cluster8_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 9\n",
    "mlist = df9['device_id'].unique().tolist()\n",
    "df9_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df9.loc[df9['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df9_r_entr = df9_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df9_r_entr.to_csv(\"Cluster9_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df9['device_id'].unique().tolist()\n",
    "mlist = mlist[:538]\n",
    "df9_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df9.loc[df9['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df9_r_entr = df9_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df9_r_entr.to_csv(\"Cluster9_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m morn \u001b[38;5;241m=\u001b[39m df7\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m26\u001b[39m:\u001b[38;5;241m43\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Morning slot: 6am to 10am\u001b[39;00m\n\u001b[0;32m     52\u001b[0m daytime \u001b[38;5;241m=\u001b[39m df7\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m46\u001b[39m:\u001b[38;5;241m58\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Daytime slot: 11am to 02pm\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m pre_peak \u001b[38;5;241m=\u001b[39m \u001b[43mdf7\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m59\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m71\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Pre-peak slot: 2pm to 5pm\u001b[39;00m\n\u001b[0;32m     54\u001b[0m peak \u001b[38;5;241m=\u001b[39m df7\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m71\u001b[39m:\u001b[38;5;241m95\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Peak slot: 5pm to 11pm\u001b[39;00m\n\u001b[0;32m     55\u001b[0m post_peak \u001b[38;5;241m=\u001b[39m df7\u001b[38;5;241m.\u001b[39miloc[:,[\u001b[38;5;241m95\u001b[39m,\u001b[38;5;241m96\u001b[39m,\u001b[38;5;241m97\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;66;03m# Post-peak slot: 11pm to 2am\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:11119\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11101\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11102\u001b[0m     _num_doc,\n\u001b[0;32m  11103\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11118\u001b[0m ):\n\u001b[1;32m> 11119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10689\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  10682\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10683\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10687\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  10688\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 10689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  10690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  10691\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:10641\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  10631\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  10633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10636\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  10637\u001b[0m     )\n\u001b[0;32m  10638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  10639\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  10640\u001b[0m     )\n\u001b[1;32m> 10641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  10643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10020\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10016\u001b[0m ignore_failures \u001b[38;5;241m=\u001b[39m numeric_only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m  10018\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  10019\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 10020\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10021\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor(res)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  10022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1389\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m   1387\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1389\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1392\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:412\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9992\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m   9990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39m_reduce(name, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   9991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 9992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py:146\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# `mask` is not recognised by bottleneck, would raise\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m#  TypeError if called\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 146\u001b[0m     result \u001b[38;5;241m=\u001b[39m bn_func(values, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# prefer to treat inf/-inf as NA, but must compute the func\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# twice :(\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_infs(result):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating the relative entropy for cluster 10\n",
    "mlist = df10['device_id'].unique().tolist()\n",
    "df10_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df10.loc[df10['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df10_r_entr = df10_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df10_r_entr.to_csv(\"Cluster10_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df10['device_id'].unique().tolist()\n",
    "mlist = mlist[:747]\n",
    "df10_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df10.loc[df10['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df10_r_entr = df10_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df10_r_entr.to_csv(\"Cluster10_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10_r_entr.to_csv(\"Cluster10_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 11\n",
    "mlist = df11['device_id'].unique().tolist()\n",
    "df11_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df11.loc[df11['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df11_r_entr = df11_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df11_r_entr.to_csv(\"Cluster11_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df11['device_id'].unique().tolist()\n",
    "mlist = mlist[:1405]\n",
    "df11_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df11.loc[df11['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df11_r_entr = df11_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df11_r_entr.to_csv(\"Cluster11_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 12\n",
    "mlist = df12['device_id'].unique().tolist()\n",
    "df12_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_d2d','r_entr_d2d_morn', 'r_entr_d2d_daytime',\n",
    "                                      'r_entr_d2d_pre-peak', 'r_entr_d2d_peak', 'r_entr_d2d_post-peak',\n",
    "                                      'r_entr_d2mean', 'r_entr_d2mean_morn', 'r_entr_d2mean_daytime', 'r_entr_d2mean_pre-peak',\n",
    "                                      'r_entr_d2mean_peak', 'r_entr_d2mean_post-peak', 'dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df12.loc[df12['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ############ Relative entropy calculation - Day to day ############\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_d2d = rel_entr(l2, l1)\n",
    "        r_entr1_d2d = r_entr_d2d.sum()\n",
    "        ############ Relative entropy calculation - Day to day for block of hours ############\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        ############ Relative entropy calculation - Day to mean\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        r_entr_d2mean = rel_entr(l1, df1_mean)\n",
    "        r_entr1_d2mean = r_entr_d2mean.sum()\n",
    "        ############ Relative entropy calculation - Day to mean for block of hours ############\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "        daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "        pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "        peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "        post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "        r_entr_morn1 = rel_entr(l1_morn, morn)\n",
    "        r_entr_daytime1 = rel_entr(l1_daytime, daytime)\n",
    "        r_entr_pre_peak1 = rel_entr(l1_pre_peak, pre_peak)\n",
    "        r_entr_peak1 = rel_entr(l1_peak, peak)\n",
    "        r_entr_post_peak1 = rel_entr(l1_post_peak, post_peak)\n",
    "        r_entr1_morn1 = r_entr_morn1.sum()\n",
    "        r_entr1_daytime1 = r_entr_daytime1.sum()\n",
    "        r_entr1_pre_peak1 = r_entr_pre_peak1.sum()\n",
    "        r_entr1_peak1 = r_entr_peak1.sum()\n",
    "        r_entr1_post_peak1 = r_entr_post_peak1.sum()\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df12_r_entr = df12_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_d2d': r_entr1_d2d,\n",
    "                                        'r_entr_d2d_morn': r_entr1_morn, 'r_entr_d2d_daytime': r_entr1_daytime,\n",
    "                                        'r_entr_d2d_pre-peak': r_entr1_pre_peak, 'r_entr_d2d_peak': r_entr1_peak, \n",
    "                                        'r_entr_d2d_post-peak': r_entr1_post_peak,'r_entr_d2mean': r_entr1_d2mean, \n",
    "                                        'r_entr_d2mean_morn': r_entr1_morn1, 'r_entr_d2mean_daytime': r_entr1_daytime1, \n",
    "                                        'r_entr_d2mean_pre-peak': r_entr1_pre_peak1, 'r_entr_d2mean_peak': r_entr1_peak1, \n",
    "                                        'r_entr_d2mean_post-peak': r_entr1_post_peak1, 'dailymean': dailymean,\n",
    "                                        'totalcons': totalcons}, ignore_index = True)\n",
    "df12_r_entr.to_csv(\"Cluster12_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "mlist = df12['device_id'].unique().tolist()\n",
    "mlist = mlist[:280]\n",
    "df12_r_entr = pd.DataFrame(columns = ['dates','idmeter','dailymean','totalcons'])\n",
    "for j in mlist:\n",
    "    df1_1 = df12.loc[df12['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        #print(j)\n",
    "        ############# Calculation of daily mean consumption and total consumption\n",
    "        df1_main = df_main.loc[df_main['device_id'] == j]\n",
    "        dailymean = df1_main.iloc[:,2:98].sum(axis=1).mean()\n",
    "        totalcons = df1_main.iloc[:,2:98].sum(axis=1).sum()\n",
    "        #print(dailymean)\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df12_r_entr = df12_r_entr.append({'dates': d1, 'idmeter': m1, 'dailymean': dailymean, 'totalcons': totalcons}, ignore_index = True)\n",
    "df12_r_entr.to_csv(\"Cluster12_RE_mean_totalcons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 1\n",
    "m1list = df1['device_id'].unique().tolist()\n",
    "df1_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m1list:\n",
    "    df1_1 = df1.loc[df1['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        l1 = df1_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df1_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df1_1_r_entr = df1_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df1_1_r_entr.to_csv(\"Cluster1_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 2\n",
    "m2list = df2['device_id'].unique().tolist()\n",
    "df2_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m2list:\n",
    "    df2_1 = df2.loc[df2['device_id'] == j]\n",
    "    for i in range(0, df2_1['dates'].nunique()-1):\n",
    "        l1 = df2_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df2_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1,  df2_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df2_1['dates'].iloc[i]\n",
    "        m1 = df2_1['device_id'].iloc[i]\n",
    "        df2_1_r_entr = df2_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df2_1_r_entr.to_csv(\"Cluster2_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3\n",
    "m3list = df3['device_id'].unique().tolist()\n",
    "df3_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m3list:\n",
    "    df3_1 = df3.loc[df3['device_id'] == j]\n",
    "    for i in range(0, df3_1['dates'].nunique()-1):\n",
    "        l1 = df3_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df3_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1,  df3_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df3_1['dates'].iloc[i]\n",
    "        m1 = df3_1['device_id'].iloc[i]\n",
    "        df3_1_r_entr = df3_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df3_1_r_entr.to_csv(\"Cluster3_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 4\n",
    "m4list = df4['device_id'].unique().tolist()\n",
    "df4_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m4list:\n",
    "    df4_1 = df4.loc[df4['device_id'] == j]\n",
    "    for i in range(0, df4_1['dates'].nunique()-1):\n",
    "        l1 = df4_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df4_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df4_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df4_1['dates'].iloc[i]\n",
    "        m1 = df4_1['device_id'].iloc[i]\n",
    "        df4_1_r_entr = df4_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df4_1_r_entr.to_csv(\"Cluster4_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 5\n",
    "m5list = df5['device_id'].unique().tolist()\n",
    "df5_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m5list:\n",
    "    df5_1 = df5.loc[df5['device_id'] == j]\n",
    "    for i in range(0, df5_1['dates'].nunique()-1):\n",
    "        l1 = df5_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df5_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df5_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df5_1['dates'].iloc[i]\n",
    "        m1 = df5_1['device_id'].iloc[i]\n",
    "        df5_1_r_entr = df5_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df5_1_r_entr.to_csv(\"Cluster5_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 6\n",
    "m6list = df6['device_id'].unique().tolist()\n",
    "df6_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m6list:\n",
    "    df6_1 = df6.loc[df6['device_id'] == j]\n",
    "    for i in range(0, df6_1['dates'].nunique()-1):\n",
    "        l1 = df6_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df6_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df6_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df6_1['dates'].iloc[i]\n",
    "        m1 = df6_1['device_id'].iloc[i]\n",
    "        df6_1_r_entr = df6_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df6_1_r_entr.to_csv(\"Cluster6_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 7\n",
    "m7list = df7['device_id'].unique().tolist()\n",
    "df7_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m7list:\n",
    "    df7_1 = df7.loc[df7['device_id'] == j]\n",
    "    for i in range(0, df7_1['dates'].nunique()-1):\n",
    "        l1 = df7_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df7_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df7_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df7_1['dates'].iloc[i]\n",
    "        m1 = df7_1['device_id'].iloc[i]\n",
    "        df7_1_r_entr = df7_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df7_1_r_entr.to_csv(\"Cluster7_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 8\n",
    "m8list = df8['device_id'].unique().tolist()\n",
    "df8_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m8list:\n",
    "    df8_1 = df8.loc[df8['device_id'] == j]\n",
    "    for i in range(0, df8_1['dates'].nunique()-1):\n",
    "        l1 = df8_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df8_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df8_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df8_1['dates'].iloc[i]\n",
    "        m1 = df8_1['device_id'].iloc[i]\n",
    "        df8_1_r_entr = df8_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df8_1_r_entr.to_csv(\"Cluster8_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 9\n",
    "m9list = df9['device_id'].unique().tolist()\n",
    "df9_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m9list:\n",
    "    df9_1 = df9.loc[df9['device_id'] == j]\n",
    "    for i in range(0, df9_1['dates'].nunique()-1):\n",
    "        l1 = df9_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df9_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df9_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df9_1['dates'].iloc[i]\n",
    "        m1 = df9_1['device_id'].iloc[i]\n",
    "        df9_1_r_entr = df9_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df9_1_r_entr.to_csv(\"Cluster9_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 10\n",
    "m10list = df10['device_id'].unique().tolist()\n",
    "df10_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m10list:\n",
    "    df10_1 = df10.loc[df10['device_id'] == j]\n",
    "    for i in range(0, df10_1['dates'].nunique()-1):\n",
    "        l1 = df10_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df10_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df10_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df10_1['dates'].iloc[i]\n",
    "        m1 = df10_1['device_id'].iloc[i]\n",
    "        df10_1_r_entr = df10_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df10_1_r_entr.to_csv(\"Cluster10_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 11\n",
    "m11list = df11['device_id'].unique().tolist()\n",
    "df11_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m11list:\n",
    "    df11_1 = df11.loc[df11['device_id'] == j]\n",
    "    for i in range(0, df11_1['dates'].nunique()-1):\n",
    "        l1 = df11_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df11_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df11_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df11_1['dates'].iloc[i]\n",
    "        m1 = df11_1['device_id'].iloc[i]\n",
    "        df11_1_r_entr = df11_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df11_1_r_entr.to_csv(\"Cluster11_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 12\n",
    "m12list = df12['device_id'].unique().tolist()\n",
    "df12_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr'])\n",
    "for j in m12list:\n",
    "    df12_1 = df12.loc[df12['device_id'] == j]\n",
    "    for i in range(0, df12_1['dates'].nunique()-1):\n",
    "        l1 = df12_1.iloc[[i],2:98].values.tolist()\n",
    "        #l2 = df12_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr = rel_entr(l1, df12_mean)\n",
    "        r_entr1 = r_entr.sum()\n",
    "        d1 = df12_1['dates'].iloc[i]\n",
    "        m1 = df12_1['device_id'].iloc[i]\n",
    "        df12_1_r_entr = df12_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr': r_entr1}, ignore_index = True)\n",
    "df12_1_r_entr.to_csv(\"Cluster12_mean_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>23:15:00</th>\n",
       "      <th>23:30:00</th>\n",
       "      <th>23:45:00</th>\n",
       "      <th>00:00:00</th>\n",
       "      <th>00:15:00</th>\n",
       "      <th>00:30:00</th>\n",
       "      <th>00:45:00</th>\n",
       "      <th>01:00:00</th>\n",
       "      <th>01:15:00</th>\n",
       "      <th>01:30:00</th>\n",
       "      <th>01:45:00</th>\n",
       "      <th>02:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.013042</td>\n",
       "      <td>0.013042</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.011179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.009043</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.008407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.010126</td>\n",
       "      <td>0.010126</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.009833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.012402</td>\n",
       "      <td>0.012569</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.011899</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.012234</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.010223</td>\n",
       "      <td>0.010558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.007622</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.011028</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.011677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121607</th>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>0.014857</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.013770</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>0.013649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121608</th>\n",
       "      <td>0.010493</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.014713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121609</th>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.014775</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>0.006935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121610</th>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.011013</td>\n",
       "      <td>0.010205</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.011114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121611</th>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.004053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18658 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        23:15:00  23:30:00  23:45:00  00:00:00  00:15:00  00:30:00  00:45:00  \\\n",
       "123     0.013042  0.013042  0.011687  0.011856  0.011179  0.011179  0.011179   \n",
       "124     0.009043  0.009171  0.009171  0.010317  0.008916  0.008789  0.008916   \n",
       "125     0.010126  0.010126  0.010273  0.010273  0.009979  0.009833  0.009833   \n",
       "126     0.012402  0.012569  0.012737  0.011899  0.012066  0.012234  0.011228   \n",
       "127     0.007622  0.007622  0.007622  0.012326  0.012163  0.011353  0.011028   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "121607  0.011233  0.011112  0.011596  0.014253  0.016910  0.014253  0.014857   \n",
       "121608  0.010493  0.010607  0.010949  0.014256  0.014370  0.014028  0.014028   \n",
       "121609  0.011156  0.011006  0.011006  0.015830  0.014775  0.010252  0.007840   \n",
       "121610  0.007073  0.005658  0.003233  0.012125  0.011013  0.010205  0.010508   \n",
       "121611  0.003200  0.003200  0.003200  0.004267  0.004267  0.004267  0.004267   \n",
       "\n",
       "        01:00:00  01:15:00  01:30:00  01:45:00  02:00:00  \n",
       "123     0.011179  0.011179  0.011179  0.011009  0.011179  \n",
       "124     0.009171  0.008407  0.008534  0.008534  0.008407  \n",
       "125     0.009833  0.009979  0.009833  0.009686  0.009833  \n",
       "126     0.010558  0.010558  0.010390  0.010223  0.010558  \n",
       "127     0.011190  0.011353  0.011353  0.011190  0.011677  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "121607  0.014011  0.014978  0.013770  0.014253  0.013649  \n",
       "121608  0.013344  0.013800  0.013002  0.013002  0.014713  \n",
       "121609  0.011458  0.007237  0.010855  0.006935  0.006935  \n",
       "121610  0.009902  0.010811  0.010710  0.010003  0.011114  \n",
       "121611  0.004267  0.003627  0.003840  0.004267  0.004053  \n",
       "\n",
       "[18658 rows x 12 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 1 considering different slots (Difference with mean).\n",
    "df1_mean_morn = df1.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df1_mean_daytime = df1.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df1_mean_pre_peak = df1.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df1_mean_peak = df1.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df1_mean_post_peak = df1.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m1list = df1['device_id'].unique().tolist()\n",
    "df1_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m1list:\n",
    "    df1_1 = df1.loc[df1['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        l1_morn = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df1_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df1_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df1_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df1_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df1_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df1_1_r_entr = df1_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df1_1_r_entr.to_csv(\"Cluster1_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 2 considering different slots (Difference with mean).\n",
    "df2_mean_morn = df2.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df2_mean_daytime = df2.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df2_mean_pre_peak = df2.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df2_mean_peak = df2.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df2_mean_post_peak = df2.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m2list = df2['device_id'].unique().tolist()\n",
    "df2_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m2list:\n",
    "    df2_1 = df2.loc[df2['device_id'] == j]\n",
    "    for i in range(0, df2_1['dates'].nunique()-1):\n",
    "        l1_morn = df2_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df2_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df2_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df2_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df2_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df2_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df2_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df2_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df2_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df2_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df2_1['dates'].iloc[i]\n",
    "        m1 = df2_1['device_id'].iloc[i]\n",
    "        df2_1_r_entr = df2_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df2_1_r_entr.to_csv(\"Cluster2_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3 considering different slots (Difference with mean).\n",
    "df3_mean_morn = df3.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df3_mean_daytime = df3.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df3_mean_pre_peak = df3.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df3_mean_peak = df3.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df3_mean_post_peak = df3.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m3list = df3['device_id'].unique().tolist()\n",
    "df3_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m3list:\n",
    "    df3_1 = df3.loc[df3['device_id'] == j]\n",
    "    for i in range(0, df3_1['dates'].nunique()-1):\n",
    "        l1_morn = df3_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df3_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df3_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df3_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df3_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df3_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df3_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df3_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df3_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df3_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df3_1['dates'].iloc[i]\n",
    "        m1 = df3_1['device_id'].iloc[i]\n",
    "        df3_1_r_entr = df3_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df3_1_r_entr.to_csv(\"Cluster3_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 4 considering different slots (Difference with mean).\n",
    "df4_mean_morn = df4.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df4_mean_daytime = df4.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df4_mean_pre_peak = df4.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df4_mean_peak = df4.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df4_mean_post_peak = df4.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m4list = df1['device_id'].unique().tolist()\n",
    "df4_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m4list:\n",
    "    df4_1 = df4.loc[df4['device_id'] == j]\n",
    "    for i in range(0, df4_1['dates'].nunique()-1):\n",
    "        l1_morn = df4_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df4_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df4_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df4_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df4_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df4_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df4_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df4_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df4_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df4_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df4_1['dates'].iloc[i]\n",
    "        m1 = df4_1['device_id'].iloc[i]\n",
    "        df4_1_r_entr = df4_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df4_1_r_entr.to_csv(\"Cluster4_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 5 considering different slots (Difference with mean).\n",
    "df5_mean_morn = df5.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df5_mean_daytime = df5.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df5_mean_pre_peak = df5.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df5_mean_peak = df5.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df5_mean_post_peak = df5.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m5list = df5['device_id'].unique().tolist()\n",
    "df5_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m5list:\n",
    "    df5_1 = df5.loc[df5['device_id'] == j]\n",
    "    for i in range(0, df5_1['dates'].nunique()-1):\n",
    "        l1_morn = df5_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df5_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df5_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df5_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df5_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df5_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df5_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df5_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df5_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df5_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df5_1['dates'].iloc[i]\n",
    "        m1 = df5_1['device_id'].iloc[i]\n",
    "        df5_1_r_entr = df5_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df5_1_r_entr.to_csv(\"Cluster5_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 6 considering different slots (Difference with mean).\n",
    "df6_mean_morn = df6.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df6_mean_daytime = df6.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df6_mean_pre_peak = df6.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df6_mean_peak = df6.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df6_mean_post_peak = df6.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m6list = df6['device_id'].unique().tolist()\n",
    "df6_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m6list:\n",
    "    df6_1 = df6.loc[df6['device_id'] == j]\n",
    "    for i in range(0, df6_1['dates'].nunique()-1):\n",
    "        l1_morn = df6_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df6_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df6_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df6_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df6_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df6_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df6_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df6_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df6_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df6_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df6_1['dates'].iloc[i]\n",
    "        m1 = df6_1['device_id'].iloc[i]\n",
    "        df6_1_r_entr = df6_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df6_1_r_entr.to_csv(\"Cluster6_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 7 considering different slots (Difference with mean).\n",
    "df7_mean_morn = df7.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df7_mean_daytime = df7.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df7_mean_pre_peak = df7.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df7_mean_peak = df7.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df7_mean_post_peak = df7.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m7list = df7['device_id'].unique().tolist()\n",
    "df7_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m7list:\n",
    "    df7_1 = df7.loc[df7['device_id'] == j]\n",
    "    for i in range(0, df7_1['dates'].nunique()-1):\n",
    "        l1_morn = df7_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df7_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df7_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df7_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df7_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df7_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df7_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df7_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df7_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df7_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df7_1['dates'].iloc[i]\n",
    "        m1 = df7_1['device_id'].iloc[i]\n",
    "        df7_1_r_entr = df7_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df7_1_r_entr.to_csv(\"Cluster7_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 8 considering different slots (Difference with mean).\n",
    "df8_mean_morn = df8.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df8_mean_daytime = df8.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df8_mean_pre_peak = df8.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df8_mean_peak = df8.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df8_mean_post_peak = df8.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m8list = df8['device_id'].unique().tolist()\n",
    "df8_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m8list:\n",
    "    df8_1 = df8.loc[df8['device_id'] == j]\n",
    "    for i in range(0, df8_1['dates'].nunique()-1):\n",
    "        l1_morn = df8_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df8_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df8_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df8_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df8_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df8_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df8_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df8_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df8_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df8_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df8_1['dates'].iloc[i]\n",
    "        m1 = df8_1['device_id'].iloc[i]\n",
    "        df8_1_r_entr = df8_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df8_1_r_entr.to_csv(\"Cluster8_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 9 considering different slots (Difference with mean).\n",
    "df9_mean_morn = df9.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df9_mean_daytime = df9.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df9_mean_pre_peak = df9.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df9_mean_peak = df9.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df9_mean_post_peak = df9.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m9list = df9['device_id'].unique().tolist()\n",
    "df9_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m9list:\n",
    "    df9_1 = df9.loc[df9['device_id'] == j]\n",
    "    for i in range(0, df9_1['dates'].nunique()-1):\n",
    "        l1_morn = df9_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df9_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df9_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df9_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df9_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df9_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df9_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df9_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df9_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df9_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df9_1['dates'].iloc[i]\n",
    "        m1 = df9_1['device_id'].iloc[i]\n",
    "        df9_1_r_entr = df9_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df9_1_r_entr.to_csv(\"Cluster9_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 10 considering different slots (Difference with mean).\n",
    "df10_mean_morn = df10.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df10_mean_daytime = df10.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df10_mean_pre_peak = df10.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df10_mean_peak = df10.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df10_mean_post_peak = df10.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m10list = df10['device_id'].unique().tolist()\n",
    "df10_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m10list:\n",
    "    df10_1 = df10.loc[df10['device_id'] == j]\n",
    "    for i in range(0, df10_1['dates'].nunique()-1):\n",
    "        l1_morn = df10_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df10_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df10_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df10_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df10_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df10_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df10_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df10_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df10_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df10_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df10_1['dates'].iloc[i]\n",
    "        m1 = df10_1['device_id'].iloc[i]\n",
    "        df10_1_r_entr = df10_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df10_1_r_entr.to_csv(\"Cluster10_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 11 considering different slots (Difference with mean).\n",
    "df11_mean_morn = df11.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df11_mean_daytime = df11.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df11_mean_pre_peak = df11.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df11_mean_peak = df11.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df11_mean_post_peak = df11.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m11list = df11['device_id'].unique().tolist()\n",
    "df11_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m11list:\n",
    "    df11_1 = df11.loc[df11['device_id'] == j]\n",
    "    for i in range(0, df11_1['dates'].nunique()-1):\n",
    "        l1_morn = df11_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df11_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df11_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df11_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df11_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df11_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df11_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df11_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df11_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df11_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df11_1['dates'].iloc[i]\n",
    "        m1 = df11_1['device_id'].iloc[i]\n",
    "        df11_1_r_entr = df11_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df11_1_r_entr.to_csv(\"Cluster11_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 7 considering different slots (Difference with mean).\n",
    "df12_mean_morn = df12.iloc[:,26:43].mean(axis=0).values.tolist() # Morning slot: 6am to 10am\n",
    "df12_mean_daytime = df12.iloc[:,46:58].mean(axis=0).values.tolist() # Daytime slot: 11am to 02pm\n",
    "df12_mean_pre_peak = df12.iloc[:,59:71].mean(axis=0).values.tolist() # Pre-peak slot: 2pm to 5pm\n",
    "df12_mean_peak = df12.iloc[:,71:95].mean(axis=0).values.tolist() # Peak slot: 5pm to 11pm\n",
    "df12_mean_post_peak = df12.iloc[:,[95,96,97,2,3,4,5,6,7,8,9,10]].mean(axis=0).values.tolist() # Post-peak slot: 11pm to 2am\n",
    "m12list = df12['device_id'].unique().tolist()\n",
    "df12_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m12list:\n",
    "    df12_1 = df12.loc[df12['device_id'] == j]\n",
    "    for i in range(0, df12_1['dates'].nunique()-1):\n",
    "        l1_morn = df12_1.iloc[[i],26:43].values.tolist()\n",
    "        l1_daytime = df12_1.iloc[[i],46:58].values.tolist()\n",
    "        l1_pre_peak = df12_1.iloc[[i],59:71].values.tolist()\n",
    "        l1_peak = df12_1.iloc[[i],71:95].values.tolist()\n",
    "        l1_post_peak = df12_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        #l2 = df1_1.iloc[[i+1],2:98].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l1_morn, df12_mean_morn)\n",
    "        r_entr_daytime = rel_entr(l1_daytime, df12_mean_daytime)\n",
    "        r_entr_pre_peak = rel_entr(l1_pre_peak, df12_mean_pre_peak)\n",
    "        r_entr_peak = rel_entr(l1_peak, df12_mean_peak)\n",
    "        r_entr_post_peak = rel_entr(l1_post_peak, df12_mean_post_peak)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df12_1['dates'].iloc[i]\n",
    "        m1 = df12_1['device_id'].iloc[i]\n",
    "        df12_1_r_entr = df12_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df12_1_r_entr.to_csv(\"Cluster12_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 1 considering different slots (Day to day difference)\n",
    "m1list = df1['device_id'].unique().tolist()\n",
    "df1_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m1list:\n",
    "    df1_1 = df1.loc[df1['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df1_1_r_entr = df1_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df1_1_r_entr.to_csv(\"Cluster1_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 2 considering different slots (Day to day difference)\n",
    "m2list = df2['device_id'].unique().tolist()\n",
    "df2_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m2list:\n",
    "    df2_1 = df2.loc[df2['device_id'] == j]\n",
    "    for i in range(0, df2_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df2_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df2_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df2_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df2_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df2_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df2_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df2_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df2_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df2_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df2_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df2_1['dates'].iloc[i]\n",
    "        m1 = df2_1['device_id'].iloc[i]\n",
    "        df2_1_r_entr = df2_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df2_1_r_entr.to_csv(\"Cluster2_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 3 considering different slots (Day to day difference)\n",
    "m3list = df3['device_id'].unique().tolist()\n",
    "df3_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m3list:\n",
    "    df3_1 = df3.loc[df3['device_id'] == j]\n",
    "    for i in range(0, df3_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df3_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df3_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df3_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df3_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df3_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df3_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df3_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df3_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df3_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df3_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df3_1['dates'].iloc[i]\n",
    "        m1 = df3_1['device_id'].iloc[i]\n",
    "        df3_1_r_entr = df3_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df3_1_r_entr.to_csv(\"Cluster3_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 4 considering different slots (Day to day difference)\n",
    "m4list = df4['device_id'].unique().tolist()\n",
    "df4_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m4list:\n",
    "    df4_1 = df4.loc[df4['device_id'] == j]\n",
    "    for i in range(0, df4_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df4_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df4_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df4_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df4_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df4_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df4_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df4_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df4_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df4_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df4_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df4_1['dates'].iloc[i]\n",
    "        m1 = df4_1['device_id'].iloc[i]\n",
    "        df4_1_r_entr = df4_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df4_1_r_entr.to_csv(\"Cluster4_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 5 considering different slots (Day to day difference)\n",
    "m5list = df5['device_id'].unique().tolist()\n",
    "df5_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m5list:\n",
    "    df5_1 = df5.loc[df5['device_id'] == j]\n",
    "    for i in range(0, df5_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df5_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df5_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df5_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df5_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df5_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df5_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df5_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df5_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df5_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df5_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df5_1['dates'].iloc[i]\n",
    "        m1 = df5_1['device_id'].iloc[i]\n",
    "        df5_1_r_entr = df5_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df5_1_r_entr.to_csv(\"Cluster5_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 6 considering different slots (Day to day difference)\n",
    "m6list = df6['device_id'].unique().tolist()\n",
    "df6_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m6list:\n",
    "    df6_1 = df6.loc[df6['device_id'] == j]\n",
    "    for i in range(0, df6_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df6_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df6_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df6_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df6_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df6_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df6_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df6_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df6_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df6_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df6_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df6_1['dates'].iloc[i]\n",
    "        m1 = df6_1['device_id'].iloc[i]\n",
    "        df6_1_r_entr = df6_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df6_1_r_entr.to_csv(\"Cluster6_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 7 considering different slots (Day to day difference)\n",
    "m7list = df7['device_id'].unique().tolist()\n",
    "df7_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m7list:\n",
    "    df7_1 = df7.loc[df7['device_id'] == j]\n",
    "    for i in range(0, df7_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df7_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df7_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df7_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df7_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df7_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df7_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df7_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df7_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df7_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df7_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df7_1['dates'].iloc[i]\n",
    "        m1 = df7_1['device_id'].iloc[i]\n",
    "        df7_1_r_entr = df7_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df7_1_r_entr.to_csv(\"Cluster7_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 8 considering different slots (Day to day difference)\n",
    "m8list = df8['device_id'].unique().tolist()\n",
    "df8_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in m8list:\n",
    "    df8_1 = df8.loc[df8['device_id'] == j]\n",
    "    for i in range(0, df8_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df8_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df8_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df8_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df8_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df8_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df8_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df8_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df8_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df8_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df8_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df8_1['dates'].iloc[i]\n",
    "        m1 = df8_1['device_id'].iloc[i]\n",
    "        df8_1_r_entr = df8_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df8_1_r_entr.to_csv(\"Cluster8_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 9 considering different slots (Day to day difference)\n",
    "mlist = df9['device_id'].unique().tolist()\n",
    "df9_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in mlist:\n",
    "    df1_1 = df9.loc[df9['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df9_1_r_entr = df9_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df9_1_r_entr.to_csv(\"Cluster9_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 10 considering different slots (Day to day difference)\n",
    "mlist = df10['device_id'].unique().tolist()\n",
    "df10_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in mlist:\n",
    "    df1_1 = df10.loc[df10['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df10_1_r_entr = df10_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df10_1_r_entr.to_csv(\"Cluster10_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 11 considering different slots (Day to day difference)\n",
    "mlist = df11['device_id'].unique().tolist()\n",
    "df11_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in mlist:\n",
    "    df1_1 = df11.loc[df11['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df11_1_r_entr = df11_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df11_1_r_entr.to_csv(\"Cluster11_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the relative entropy for cluster 12 considering different slots (Day to day difference)\n",
    "mlist = df12['device_id'].unique().tolist()\n",
    "df12_1_r_entr = pd.DataFrame(columns = ['dates','idmeter','r_entr_morn', 'r_entr_daytime',\n",
    "                                       'r_entr_pre-peak', 'r_entr_peak', 'r_entr_post-peak'])\n",
    "for j in mlist:\n",
    "    df1_1 = df12.loc[df12['device_id'] == j]\n",
    "    for i in range(0, df1_1['dates'].nunique()-1):\n",
    "        ####### Morning slot ########\n",
    "        l1 = df1_1.iloc[[i],26:43].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],26:43].values.tolist() # To cacluate the relative entropy between days\n",
    "        r_entr_morn = rel_entr(l2, l1)\n",
    "        r_entr1_morn = r_entr_morn.sum()\n",
    "        ####### Daytime slot ########\n",
    "        l1 = df1_1.iloc[[i],46:58].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],46:58].values.tolist()\n",
    "        r_entr_daytime = rel_entr(l2, l1)\n",
    "        r_entr1_daytime = r_entr_daytime.sum()\n",
    "        ####### Pre-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],59:71].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],59:71].values.tolist()\n",
    "        r_entr_pre_peak = rel_entr(l2, l1)\n",
    "        r_entr1_pre_peak = r_entr_pre_peak.sum()\n",
    "        ####### Peak slot ########\n",
    "        l1 = df1_1.iloc[[i],71:95].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],71:95].values.tolist()\n",
    "        r_entr_peak = rel_entr(l2, l1)\n",
    "        r_entr1_peak = r_entr_peak.sum()\n",
    "        ####### Post-peak slot ########\n",
    "        l1 = df1_1.iloc[[i],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        l2 = df1_1.iloc[[i+1],[95,96,97,2,3,4,5,6,7,8,9,10]].values.tolist()\n",
    "        r_entr_post_peak = rel_entr(l2, l1)\n",
    "        r_entr1_post_peak = r_entr_post_peak.sum()\n",
    "        d1 = df1_1['dates'].iloc[i]\n",
    "        m1 = df1_1['device_id'].iloc[i]\n",
    "        df12_1_r_entr = df12_1_r_entr.append({'dates': d1, 'idmeter': m1, 'r_entr_morn': r_entr1_morn,\n",
    "                                           'r_entr_daytime': r_entr1_daytime, 'r_entr_pre-peak': r_entr1_pre_peak,\n",
    "                                           'r_entr_peak': r_entr1_peak, 'r_entr_post-peak': r_entr1_post_peak}, \n",
    "                                           ignore_index = True)\n",
    "df12_1_r_entr.to_csv(\"Cluster12_daytoday_blockwise_RE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
